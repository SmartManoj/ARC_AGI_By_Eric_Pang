# Run with: python src/train.py --config-path configs/arc_train_scaling --config-name 400

training:
  seed: 300
  resume_from_checkpoint: null  # null to start from scratch
  inference_mode: gradient_ascent  # mean, all, random_search, gradient_ascent
  inference_kwargs:
    num_steps: 1
    lr: 0.3
    lr_schedule: True
    optimizer: adam
    optimizer_kwargs:
      b2: 0.9
    stop_gradient_latent_move: True
  batch_size: 128  # this has to be divisible by gradient_accumulation_steps * num_devices
  gradient_accumulation_steps: 32  # the higher the slower but the lower memory usage while keeping effective batch size constant
  total_num_steps: 400000
  log_every_n_steps: 50  # this has to respect dataset_size >= batch_size * log_every_n_steps
  eval_every_n_logs: 20  # null to disable eval
  save_checkpoint_every_n_logs: 20  # null to disable checkpointing
  learning_rate: 4e-4
  prior_kl_coeff: 1e-4
  pairwise_kl_coeff: null
  mixed_precision: False  # if True, it uses bfloat16 for activations (params stay in float32)
  online_data_augmentation: True
  task_generator:
    num_workers: 64
    num_pairs: 4
    class: ARC
    only_n_tasks: 400
  train_datasets:


eval:
  eval_datasets:
  test_datasets:
    # - generator: ARC
    #   task_generator_kwargs:
    #   name: arc_train_all_mean
    #   num_pairs: 4
    #   length: 128
    #   batch_size: 16
    #   num_tasks_to_show: 48
    # - generator: ARC
    #   task_generator_kwargs:
    #     only_n_tasks: ${training.task_generator.only_n_tasks}
    #   name: arc_train_mean
    #   num_pairs: 4
    #   length: 128
    #   batch_size: 16
    #   num_tasks_to_show: 48
  json_datasets:
    # - challenges: json/arc-agi_training_challenges.json
    #   solutions: json/arc-agi_training_solutions.json
    #   name: mean
    #   only_n_tasks: ${training.task_generator.only_n_tasks}
    #   num_tasks_to_show: 100


encoder_transformer:
  _target_: src.models.utils.EncoderTransformerConfig
  max_rows: 30
  max_cols: 30
  num_layers: 4
  transformer_layer:
    _target_: src.models.utils.TransformerLayerConfig
    num_heads: 8
    emb_dim_per_head: 32
    mlp_dim_factor: 4.0
    dropout_rate: 0.0
    attention_dropout_rate: 0.0
  latent_dim: 64
  variational: True
  latent_projection_bias: False


decoder_transformer:
  _target_: src.models.utils.DecoderTransformerConfig
  max_rows: 30
  max_cols: 30
  num_layers: 8
  transformer_layer:
    _target_: src.models.utils.TransformerLayerConfig
    num_heads: 8
    emb_dim_per_head: 32
    mlp_dim_factor: 4.0
    dropout_rate: 0.0
    attention_dropout_rate: 0.0
